{\bf IS PER TIME STEP SCALING APPROPRIATE FOR A COMPUTER TARGET
AUDIENCE? ARE WE BEING CLEAR WHEN WE SWITCH FROM FITTING OF A TIME
SERIES TO PER TIME STEP QUANTITIES?}

The computational efficiency of each algorithm concerns both the
memory used by the algorithm and the number of computational steps
required for the algorithm to execute, which is a measure of the
speed. To characterize the operational efficiency of a digital fractor
operating as a circuit element in real time, the quantities of
interest are the memory or number of processor steps {\em per time
  step}.

There are three phases of the computation that factor into the
computational cost of the binned Grunwald algorithm: initialization
(including summing the binned Grunwald weights), updating the average
values of the stored history in the bins as new input data is read,
and summing the modified Grunwald differintegral.

During intialization, several things must happen. The array defining
the binning structure, $b_k$, must be set. The history of the input
signal must be zeroed and the first output must be set to zero as
well. Most significantly, the binned weights must be calculated for
the chosen binning structure.

The time it takes to calculate or set $b_k$ is moderately dependent
upon the details of the binning structure chosen. However, any binning
structure will have a processing time that scales as $N_b$ rather than
$N_t$ because of the number of bins that must be set. Here $N_t$ is
the maximum history depth in time-steps of the $N_b$ memory storage
bins. By definition,

\begin{equation}
N_t = \displaystyle\sum_{k=0}^{N_b}b_k.
\label{eqn:Nt}
\end{equation}
On the other hand, since each of the $N_t$ individual Grunwald weights
must be summed, the time for initialization of the weights scales as
$N_t$. Typical values of $N_t$ are of order $1,000$ to $10^6$. Typical
values of $N_b$ are $10$ to $26$. Since $N_t>>N_b$, the bulk of the
initialization process occurs during the computation of the weights
and the rest of the initialization process may be neglected. The time
for intialization as a whole scales as $N_t$.

By the very design of the algorithm, there is no need to store the
full Grunwald weights. The largest arrays that need to be stored are
the binned weights and the bin capacity, each of which has size
$N_b$. As a result, memory scales as $N_b$ rather than
$N_t$. Equation~\ref{eqn:sumWk} gives the algorithm for the calculation
of the bin weights. The following recursion formula for Grunwald
weights can be used to iterate $w_j$ to a new value with each time
step. In the $j$th time step, $w_j$ has the value

\begin{equation}
w_j = \frac{j-1-\alpha}{j}w_{j-1}.
\label{eqn:GrunwaldRecursion}
\end{equation}

In the implementation of an algorithm for initialization of the
weights, individual Grunwald weights $w_j$ need not be stored in an
array after calculation. These individual weights can be summed
immediately into the corresponding bin weights $W_k$ according to
Equation~\ref{eqn:sumWk}, at which time the value of $w_j$ may be safely
overwritten with $w_{j+1}$.

The time required to update the binned history values when new input
data is read scales as $N_b$ because there are $N_b$ bins that must be
updated. Equation~\ref{eqn:updating} may be used as the basis for an
algorithm to accomplish this goal. Care must be taken to account for
the filling of the bins until the approximation reaches steady-state.

Again, a memory saving opportunity is available. Since only one bin
can be filling at a time and the rest must be either full ($c_k=b_k$)
or empty ($c_k=0$), the full array of $N_b$ occupancy numbers $c_k$
need not be stored. Instead, it is sufficient to store the index of
the bin that is filling, $k_{filling}$ and its occupancy,
$c_{filling}$. These unfilled bins with $k>k_{filling}$ and $c_k=0$
are not updated when a new input signal value is read into the
history. 

In the C++ code reported in this paper, the full array of $c_{k}$ is
stored. This does not impact the memory scaling of the updating
algorithm. Updating the stored bin history values scales as $N_b$ in
memory because the largest arrays that need to be stored are the
binned average values, the bin capacity, and the bin occupancy, each
of size $N_b$.

The two real-time operation stages, updating and differintegrating,
together require less than $400$ processor steps for $26$ bins of
history. For $26$ bins, less than $100$ memory registers are
required. Initialization, on the other hand, is very
resource-consuming. $N_t$ may be larger than a million and still not
fill $26$ bins. Initialization therefore may require more than a
million processor steps.
